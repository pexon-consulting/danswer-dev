{
    "LLM Key Setup": "LLM Einrichtung",
    "Custom": "Eigene",
    "Please setup an LLM below in order to start using Danswer Search or Danswer Chat. Don't worry, you can always change this later in the Admin Panel.": "Bitte richten Sie unten ein LLM ein, um Danswer Search oder Danswer Chat nutzen zu können. Keine Sorge, Sie können dies später jederzeit im Admin-Panel ändern",
    "Or if you'd rather look around first,": "Wenn Sie sich lieber erst einmal umschauen möchten,",
    "skip this step": "überspringen Sie diesen Schritt",
    "Display Name": "Anzeigename",
    "A name which you can use to identify this provider when selecting it in the UI.": "Der Name, mit dem Sie diesen Provider identifizieren können, wenn Sie ihn in der Benutzeroberfläche auswählen.",
    "API Key": "API Schlüssel",
    "[Optional] API Key": "[Optional] API Schlüssel",
    "Should be one of the providers listed at": "Sollte einer der hier aufgelisteten Providers sein:",
    "Name of the custom provider": "Name vom eigenen Provider",
    "API Base": "API URL",
    "[Optional] API URL": "[Optional] API URL",
    "The model to use by default for this provider unless otherwise specified.": "Das Standard-Modell für diesen Provider, sofern nicht anders angegeben.",
    "The model to use for lighter flows like `LLM Chunk Filter` for this provider. If `Default` is specified, will use the Default Model configured above.": "Das Modell, das für leichtere Abfragen, wie „LLM Chunk Filter“ für diesen Provider verwendet werden soll. Wenn „Default“ angegeben ist, wird das oben konfigurierte Standardmodell verwendet.",
    "Test successful! LLM provider is ready to go.": "Test erfolgreich! Ihr LLM Provider ist jetzt bereit.",
    "Fill in the following as is needed. Refer to the LiteLLM documentation for the model provider name specified above in order to determine which fields are required.": "Füllen Sie nach Bedarf Folgendes aus. Sehen Sie sich die LiteLLM-Dokumentation für den oben angegebenen Modellanbieternamen an, um festzustellen, welche Felder erforderlich sind.",
    "[Optional] Custom Configs": "[Optional] Eigene Konfiguration",
    "Additional configurations needed by the model provider. Are passed to litellm via environment variables.": "Zusätzliche Konfigurationen, die vom Modell Provider benötigt werden. Diese werden über Umgebungsvariablen an litellm übergeben",
    "For example, when configuring the Cloudflare provider, you would need to set `CLOUDFLARE_ACCOUNT_ID` as the key and your Cloudflare account ID as the value.": "Wenn Sie beispielsweise den Cloudflare-Provider konfigurieren, müssen Sie „CLOUDFLARE_ACCOUNT_ID“ als Schlüssel und Ihre Cloudflare-Konto-ID als Wert festlegen.",
    "Key": "Schlüssel",
    "Value": "Wert",
    "Add New": "Hinzufügen",
    "Model Names": "Modelle",
    "Default Model": "Standard Modell",
    "List the individual models that you want to make available as a part of this provider. At least one must be specified. As an example, for OpenAI one model might be 'gpt-4'.": "Listen Sie die einzelnen Modelle auf, die Sie im Rahmen dieses Anbieters zur Verfügung stellen möchten. Mindestens einer muss angegeben werden. Beispielsweise könnte ein Modell für OpenAI „gpt-4“ sein.",
    "The model to use by default for this provider unless otherwise specified. Must be one of the models listed above.": "Das Standard-Modell für diesen Provider, sofern nicht anders angegeben. Muss eines der oben gelisteten Modelle sein.",
    "Delete": "Löschen",
    "Choose Model": "Modell auswählen",
    "Override the default model for the": "Überschreiben Sie das Standardmodell für den",
    "assistant. The override will apply only for this chat session.": "Assistenten. Die Überschreibung gilt nur für diese Chatsitzung.",
    "Temperature": "Temperatur",
    "Adjust the temperature of the LLM. Higher temperature will make the LLM generate more creative and diverse responses, while lower temperature will make the LLM generate more conservative and focused responses.": "Passen Sie die Temperatur des LLM an. Eine höhere Temperatur führt dazu, dass das LLM kreativere und vielfältigere Antworten generiert, während eine niedrigere Temperatur konservativere und fokussiertere Antworten erzeugt.",
    "Display Name is required": "Anzeigename ist erforderlich",
    "Model name is required": "Modell ist erforderlich",
    "Provider Name is required": "Provider Name ist erforderlich",
    "API Key is required": "API Schlüssel ist erforderlich",
    "API Base is required": "API URL ist erforderlich",
    "API Version is required": "API Version ist erforderlich"
}